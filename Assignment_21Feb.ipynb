{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef561fdd-0d63-4d62-9d08-0f71e52f8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "\n",
    "## Web scraping is the automated process of extracting data from websites using software or scripts.\n",
    "## This data can then be stored and analyzed for various purposes.\n",
    "\n",
    "## Web scraping is used to,\n",
    "## Collect Large Amounts of Data: Automates data collection from the web, saving time and effort.\n",
    "## Monitor Competitors: Gathers data on competitor pricing, product offerings, and market trends.\n",
    "## Aggregate Information: Combines data from multiple sources for comprehensive analysis.\n",
    "\n",
    "## Web scraping is used in, \n",
    "## E-commerce: To track product prices, reviews, and availability.\n",
    "## Real Estate: To gather property listings, prices, and market trends.\n",
    "## Research and Journalism: To collect data for news stories, reports, and academic research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5208d965-1bd2-4ea6-8d51-414f8cee332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What are the different methods used for Web Scraping?\n",
    "\n",
    "## HTML Parsing: Extracts data directly from the HTML structure using libraries like BeautifulSoup in Python.\n",
    "\n",
    "## DOM Manipulation: Uses JavaScript to interact with and extract data from the Document Object Model (DOM),\n",
    "## often with tools like Puppeteer or Selenium.\n",
    "\n",
    "## API Calls: Retrieves data directly from web APIs, which are designed to provide data in a structured format\n",
    "## like JSON or XML.\n",
    "\n",
    "## Regular Expressions: Uses pattern matching to identify and extract specific data from web content.\n",
    "\n",
    "## Web Scraping Frameworks: Utilizes comprehensive tools like Scrapy to manage the entire scraping process,\n",
    "## from data extraction to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a665745-5b87-432d-a973-1e5fd1adb02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is Beautiful Soup? Why is it used?\n",
    "\n",
    "## Beautiful Soup is a Python library used for parsing HTML and XML documents.\n",
    "## It creates a parse tree from page source code, making it easy to extract and manipulate data.\n",
    "\n",
    "## Beautiful Soup is used to,\n",
    "## Navigate HTML/XML Trees: Simplifies the process of searching and navigating through HTML or XML tags.\n",
    "## Extract Data: Efficiently extracts data from web pages for web scraping projects.\n",
    "## Handle Inconsistent HTML: Gracefully handles poorly formatted or incomplete HTML,\n",
    "## making it robust for various web scraping tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4d41f6-41d9-4fcf-9d5a-07ad95b7c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Why is flask used in this Web Scraping project?\n",
    "\n",
    "## Flask is easy to set up and use, making it ideal for quickly developing web applications.\n",
    "## Flask provides a straightforward way to define routes, which helps in organizing the web scraping tasks\n",
    "## and presenting the scraped data via a web interface.\n",
    "## Flask can be easily integrated with MongoDB using libraries like PyMongo, allowing\n",
    "## efficient storage and retrieval of scraped data.\n",
    "## Flask applications are easy to deploy on various platforms, including AWS,\n",
    "## ensuring that your project can be made accessible over the web.\n",
    "## Flask's modular design allows to add functionalities such as handling user requests, presenting data,\n",
    "## and managing background scraping tasks with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfbab90c-617b-4f2a-bee4-f1d910d0bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service\n",
    "\n",
    "## AWS CodePipeline: Automates the CI/CD pipeline, orchestrating the deployment workflow of Flask application.\n",
    "## AWS Elastic Beanstalk: Hosts and manages Flask application, handling deployment,\n",
    "## scaling, and load balancing without manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eefd41-9fb7-42c4-9e21-a5fcce2cf45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b050ed-751d-439c-80c5-2e3ee0a78aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
